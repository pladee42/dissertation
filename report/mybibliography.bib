@misc{BabyLM-NA2,
  author       = {{UoS AI Team NA2}},
  title        = {BabyLM-Gemma2-Llama4-OLMo2},
  year         = {2025},
  publisher    = {GitHub},
  url          = {https://github.com/BabyLM-NA2/BabyLM-Gemma2-Llama4-OLMo2}
}

@article{Gemma2_2024,
  author       = {{Gemma Team and Google DeepMind}},
  title        = {Gemma 2: Improving Open Language Models at a Practical Size},
  year         = {2024},
  month        = {July},
  journal      = {arXiv preprint arXiv:2408.00118},
  eprint       = {2408.00118},
  archiveprefix = {arXiv},
  url          = {https://arxiv.org/abs/2408.00118}
}

@misc{Llama4_2025,
  author       = {{Meta AI}},
  title        = {The Llama 4 herd: The beginning of a new era of natively multimodal intelligence},
  year         = {2025},
  month        = {April},
  howpublished = {Meta AI Blog},
  url          = {https://ai.meta.com/blog/llama-4-multimodal-intelligence/}
}

@article{Olmo2_2025,
  author       = {Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Peter Clark and Noah A. Smith and Hannaneh Hajishirzi and Luca Soldaini and David Wadden and Kyle Lo and Matthew E. Peters and Nathan Lambert and Jesse Dodge and Sachin Kumar and Valentina Pyatkin and Sameer Singh and Luke Zettlemoyer and Mohammad Shoeybi and Tom Hope and Matt Gardner and Arman Cohan},
  title        = {2 {OLMo} 2 Furious},
  year         = {2025},
  month        = {January},
  journal      = {arXiv preprint arXiv:2501.00656},
  eprint       = {2501.00656},
  archiveprefix = {arXiv},
  url          = {https://arxiv.org/abs/2501.00656}
}

@misc{BabyLM2024eval,
  author       = {{BabyLM Challenge Organizers}},
  title        = {evaluation-pipeline-2024},
  year         = {2024},
  publisher    = {GitHub},
  url = {https://github.com/babylm/evaluation-pipeline-2024}
}

@article{Ivanova2024ewok,
  author       = {Anna A. Ivanova and Aalok Sathe and Benjamin Lipkin and Unnathi Kumar and Setayesh Radkani and Thomas H. Clark and Carina Kauf and Jennifer Hu and R.T. Pramod and Gabriel Grand and Vivian Paulun and Maria Ryskina and Ekin Aky√ºrek and Ethan Wilcox and Nafisa Rashid and Leshem Choshen and Roger Levy and Evelina Fedorenko and Joshua Tenenbaum and Jacob Andreas},
  title        = {Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models},
  year         = {2024},
  month        = {May},
  journal      = {arXiv preprint arXiv:2405.09605},
  eprint       = {2405.09605},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  url          = {https://arxiv.org/abs/2405.09605}
}

@article{Warstadt2020blimp,
  author       = {Alex Warstadt and Alicia Parrish and Haokun Liu and Anhad Mohananey and Wei Peng and Sheng-Fu Wang and Samuel R. Bowman},
  title        = {{BLiMP}: {T}he {B}enchmark of {L}inguistic {M}inimal {P}airs for {E}nglish},
  journal      = {Transactions of the Association for Computational Linguistics},
  volume       = {8},
  pages        = {377--392},
  year         = {2020},
  doi          = {10.1162/tacl_a_00321},
  url          = {https://aclanthology.org/2020.tacl-1.25}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@misc{BabyLM2023eval,
  author       = {{BabyLM Challenge Organizers}},
  title        = {evaluation-pipeline-2023},
  year         = {2023},
  publisher    = {GitHub},
  url          = {https://github.com/babylm/evaluation-pipeline-2023}
}

@misc{sanh2020distilbertdistilledversionbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.01108}, 
}

@misc{jiao2020tinybertdistillingbertnatural,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.10351}, 
}

@misc{sun2020mobilebertcompacttaskagnosticbert,
      title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices}, 
      author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
      year={2020},
      eprint={2004.02984},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.02984}, 
}

@misc{timiryasov2023babyllamaknowledgedistillation,
      title={Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty}, 
      author={Inar Timiryasov and Jean-Loup Tastet},
      year={2023},
      eprint={2308.02019},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.02019}, 
}

@misc{peng2023rwkvreinventingrnnstransformer,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13048}, 
}

@article{BabyLM-NA2,
  title={Comparing Modern Architecture Families in Data-Efficient Language Model Training},
  author={Team NA2},
  journal={BabyLM Challenge Proceedings},
  year={2025},
  volume={1},
  pages={1--15}
}

@article{Warstadt2020blimp,
  title={BLiMP: The Benchmark of Linguistic Minimal Pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press}
}

@article{Ivanova2024ewok,
  title={EWoK: Evaluating World Knowledge in Language Models},
  author={Ivanova, Anna and Talmor, Alon and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2401.06559},
  year={2024}
}

@article{Kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{OpenAI2023gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{Chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{Hoffmann2022chinchilla,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{Sorscher2022beyond,
  title={Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15905--15918},
  year={2022}
}

@article{Clark2020electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  journal={International Conference on Learning Representations},
  year={2020}
}

@article{Gururangan2020dont,
  title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@article{Tay2022ul2,
  title={UL2: Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@article{Wang2021gpt3,
  title={GPT Understands, Too},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{Linzen2020accelerated,
  title={Accelerated Learning in the Laboratory: The Case for Natural Language},
  author={Linzen, Tal},
  journal={Trends in Cognitive Sciences},
  volume={24},
  number={6},
  pages={437--448},
  year={2020},
  publisher={Elsevier}
}

@article{McCoy2020berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R. Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2020}
}

@misc{samuel2023trained100millionwords,
      title={Trained on 100 million words and still in shape: BERT meets British National Corpus}, 
      author={David Samuel and Andrey Kutuzov and Lilja √òvrelid and Erik Velldal},
      year={2023},
      eprint={2303.09859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.09859}, 
}