\chapter*{\Large \center Abstract}

% Guidance of how to write an abstract/summary provided by Nature: https://cbs.umn.edu/sites/cbs.umn.edu/files/public/downloads/Annotated_Nature_abstract.pdf

Multi-agent artificial intelligence systems have emerged as a promising paradigm for complex natural language generation tasks, enabling collaborative problem-solving through specialized agent architectures \cite{yan2025beyond_selftalk, guo2024llm_multiagent}. Automated email generation faces significant challenges in achieving human-like quality while maintaining consistency across different optimization strategies \cite{zhang2019email_subject, chen2019gmail_smart_compose}. Direct Preference Optimization (DPO) shows promise for aligning language models with human preferences, yet its effectiveness within multi-agent frameworks remains unexplored \cite{rafailov2023dpo, muldrew2024active_preference}. Existing evaluation methodologies suffer from significant limitations in objectivity and consistency when comparing multiple model variants, particularly lacking standardized assessment frameworks for multi-agent systems \cite{ye2024justice_prejudice, gu2024llm_judge_survey}. Here we show that a three-agent architecture comprising Email Generator, Checklist Creator, and Judge Agent achieves statistically equivalent performance across three DPO variants—Baseline, DPO-Synthetic, and DPO-Hybrid—when evaluated on 50 unseen validation topics ($F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$), demonstrating robustness in maintaining consistent email generation quality regardless of optimization strategy. This statistical equivalence reveals fundamental limitations in DPO effectiveness when constrained by small training datasets (400-425 preference pairs), suggesting dataset size may be more critical than optimization sophistication. The novel Hybrid prompting strategy with reasoning models successfully overcame traditional evaluation biases, establishing a replicable framework for objective multi-agent system assessment. These findings challenge assumptions about the necessity of complex DPO variants and provide evidence that robust multi-agent architectures maintain consistent performance regardless of underlying optimization complexity. For automated content generation, these results suggest architectural robustness may be more crucial than sophisticated optimization techniques when working with constrained datasets, reshaping resource prioritization between system design and data optimization. These findings mark a significant evolution in automated content generation research, demonstrating that sophisticated multi-agent architectures achieve consistent performance independent of optimization complexity, shifting focus from algorithmic sophistication to architectural robustness \cite{ferrag2025llm_autonomous_agents, liu2024advances_foundation_agents}. The established methodology represents a transferable framework applicable to diverse collaborative AI assessment challenges while providing guidance for future multi-agent system development \cite{masterman2024landscape_emerging, sapkota2025ai_agents_agentic}.