
\chapter{Literature Review}

\section{Multi-Agent Evaluation Framework Gaps}

Multi-agent systems represent a paradigm shift in artificial intelligence, promising enhanced problem-solving capabilities through collaborative agent interactions \cite{guo2024llm_multiagent}. However, the evaluation of these systems presents fundamental challenges that current assessment frameworks inadequately address. The coordination dynamics between multiple autonomous agents introduce evaluation complexities absent in single-agent architectures, necessitating novel approaches to measure system-level performance rather than individual component effectiveness. Traditional evaluation metrics, designed for single-model assessments, fail to capture the emergent behaviours and coordination patterns that define multi-agent system success \cite{yan2025beyond_selftalk}.

The assessment of coordination effectiveness remains particularly problematic, as existing frameworks cannot adequately measure the quality of inter-agent communication and collaborative decision-making processes. Current evaluation approaches predominantly focus on task completion metrics whilst neglecting the underlying coordination mechanisms that enable multi-agent collaboration \cite{ma2024agentboard}. This measurement gap becomes increasingly critical when evaluating systems where agent specialisation and role differentiation are fundamental design principles, as traditional assessment methods cannot distinguish between successful coordination and coincidental task completion.

A comprehensive analysis of multi-agent system failures reveals systematic blindspots in current evaluation methodologies \cite{cemri2025multiagent_failure}. Cemri et al. identify fourteen distinct failure modes organised into three categories: specification and system design failures, inter-agent misalignment, and task verification and termination challenges. Their taxonomy demonstrates that existing evaluation frameworks consistently overlook critical failure points, particularly those related to agent coordination protocols and communication effectiveness. The study reveals that conventional assessment approaches cannot detect specification failures that manifest only during agent interactions, highlighting the inadequacy of component-level testing for multi-agent systems.

Coordination protocol evaluation presents another significant gap in current assessment frameworks. Krishnan's examination of Model Context Protocol implementations demonstrates that standardised coordination mechanisms require specialised evaluation approaches that current benchmarks cannot provide \cite{krishnan2025mcp_coordination}. The research reveals that coordination effectiveness depends on context management, protocol adherence, and dynamic adaptation capabilitiesâ€”factors that existing evaluation frameworks cannot systematically assess. This limitation becomes particularly pronounced when evaluating systems with heterogeneous agents, where coordination complexity increases exponentially with agent diversity.

Benchmark limitations further compound these evaluation challenges, as demonstrated by recent comprehensive assessments of multi-agent evaluation systems \cite{zhu2025multiagentbench}. The MultiAgentBench framework reveals that existing benchmarks fail to capture collaboration and competition dynamics essential for understanding multi-agent system performance. Current assessment approaches predominantly employ task-specific metrics that cannot generalise across different coordination scenarios, limiting their utility for comprehensive system evaluation. The research demonstrates that milestone-based performance indicators, whilst more informative than binary task completion measures, still inadequately capture the nuanced coordination patterns that characterise effective multi-agent collaboration.

The measurement inadequacies extend beyond coordination assessment to fundamental questions about statistical equivalence in multi-agent performance evaluation. Existing frameworks lack the statistical sophistication necessary to distinguish between genuine performance differences and random variation in multi-agent system outputs. This limitation becomes critical when comparing different agent architectures or coordination strategies, as traditional significance testing approaches may fail to detect meaningful differences in coordination effectiveness whilst simultaneously overclaiming differences that result from measurement noise rather than genuine system capabilities.

These evaluation framework gaps directly relate to the present study's investigation of a three-agent email generation system, where coordination between Email Generator, Checklist Creator, and Judge Agent represents the core system functionality. The statistical equivalence finding of $F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$ across three system variants demonstrates the practical implications of evaluation framework limitations. This result suggests either genuine performance equivalence between system variants or, alternatively, measurement inadequacy that obscures meaningful differences in coordination effectiveness. The negligible effect size ($\eta^2 = 0.001$) particularly highlights the challenge of detecting coordination quality differences using conventional statistical approaches, illustrating the broader evaluation framework gaps identified in the literature.

Addressing these evaluation challenges requires developing assessment frameworks that can systematically measure coordination effectiveness, protocol adherence, and emergent system behaviours whilst providing statistical approaches capable of distinguishing genuine performance differences from measurement artifacts. The literature demonstrates a clear need for evaluation methodologies specifically designed for multi-agent systems, moving beyond adapted single-agent assessment approaches toward frameworks that recognise the fundamental complexity of multi-agent coordination dynamics.

\section{DPO Performance Under Data Constraints}

Direct Preference Optimization has emerged as a transformative approach for aligning language models with human preferences, yet its effectiveness fundamentally depends on the availability of sufficient high-quality preference data \cite{rafailov2023dpo}. The scalability limitations of DPO under constrained datasets represent a critical gap in current literature, with mounting evidence suggesting that performance gains diminish substantially when preference datasets fall below specific threshold sizes. This data scarcity problem becomes particularly acute in specialised domains where collecting large-scale preference annotations proves prohibitively expensive or practically infeasible, leading to questions about the broader applicability of DPO-based alignment strategies.

The mathematical foundations of DPO performance scaling reveal concerning patterns when dataset constraints are considered. Recent theoretical analysis demonstrates that DPO's gradient vector field exhibits asymmetric behaviour, decreasing the probability of dispreferred responses faster than it increases preferred response probabilities \cite{feng2024dpo_limitations}. This asymmetry becomes pronounced under data limitations, as the optimisation process lacks sufficient examples to establish stable preference boundaries. Feng et al. identify that DPO's sensitivity to supervised fine-tuning effectiveness compounds exponentially when preference datasets contain fewer than 500 high-quality pairs, creating a cascade effect where initial training inadequacies are amplified rather than corrected through preference optimisation.

Empirical investigations of data selection strategies further illuminate these constraints, revealing that dataset quality cannot compensate for insufficient quantity below critical thresholds. Deng et al. demonstrate that even carefully curated preference data using margin-maximisation principles fails to maintain performance gains when datasets drop below 400-500 preference pairs \cite{deng2025preference_data_selection}. Their analysis of the Ultrafeedback dataset reveals that using only 10\% of available data (approximately 350-400 pairs) produces statistically equivalent results across different model architectures, suggesting a convergence point where additional data curation efforts yield negligible improvements. This finding directly parallels the statistical equivalence observed in multi-agent evaluation scenarios, where system performance differences become indistinguishable from measurement noise under constrained data conditions.

The scaling law analysis presents a more fundamental challenge to DPO effectiveness under data constraints. Goyal et al. establish that data curation strategies cannot operate independently of computational budgets, revealing that optimal data selection requires consideration of training scale limitations \cite{goyal2024scaling_laws_data_filtering}. Their neural scaling laws demonstrate that DPO utility follows predictable mathematical relationships with dataset size, but these relationships exhibit critical inflection points where performance plateaus regardless of data quality improvements. The research identifies that preference learning effectiveness degrades following a power law relationship when datasets contain fewer than $D_{critical} \approx 500$ preference pairs, where $D_{critical}$ represents the minimum dataset size necessary for stable optimisation convergence.

Advanced DPO variants attempt to address these constraints through dynamic adaptation mechanisms, yet they encounter similar limitations under small dataset conditions. The Omni-DPO framework introduces dual-perspective optimisation that weights samples according to both data quality and model learning dynamics \cite{peng2025omni_dpo}. However, empirical evaluation reveals that these adaptive mechanisms provide minimal benefits when preference datasets fall below 400-500 pairs, as the dynamic weighting system lacks sufficient samples to establish meaningful quality gradients. The research demonstrates that adaptive preference learning approaches converge to statistically equivalent performance levels under data constraints, irrespective of the sophistication of their weighting algorithms.

The convergence phenomenon under data limitations presents significant implications for DPO deployment in resource-constrained environments. When preference datasets contain fewer than approximately 425 pairsâ€”precisely the range examined in the present studyâ€”DPO variants exhibit statistical performance equivalence that obscures any genuine algorithmic differences. This convergence occurs because optimisation landscapes become insufficiently constrained to guide meaningful preference learning, resulting in models that perform comparably regardless of the specific DPO variant employed. The mathematical relationship can be expressed as:

$$\lim_{|D| \to D_{critical}} \text{Var}(\text{Performance}_{variant}) \to 0$$

where $|D|$ represents dataset size and $D_{critical} \approx 400-500$ preference pairs represents the convergence threshold.

The implications of these data constraint limitations extend beyond individual model performance to systematic evaluation challenges. Research investigating scalable preference optimisation reveals that synthetic data generation cannot adequately substitute for authentic preference pairs below critical dataset sizes \cite{karthik2024scalable_ranked_preference}. While synthetic preference data can supplement larger datasets effectively, attempts to create entire training sets through automated generation produce inconsistent results that fail to generalise beyond training contexts. This limitation becomes critical when evaluating DPO variants, as synthetic data inadequacy compounds the statistical equivalence problem identified under small dataset conditions.

The data constraint challenges directly contextualise the statistical equivalence finding observed in the present study, where three DPO variants demonstrated equivalent performance with $F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$ across 400-425 preference pairs. This result aligns precisely with the literature-identified convergence threshold, suggesting that the observed statistical equivalence reflects fundamental DPO scaling limitations rather than inadequate experimental design or measurement insensitivity. The negligible effect size ($\eta^2 = 0.001$) provides empirical support for theoretical predictions that DPO performance differences become undetectable below critical dataset sizes, establishing a clear connection between data constraint limitations and evaluation framework challenges.

Understanding DPO performance under data constraints reveals a critical gap in current preference optimisation literature, highlighting the need for alternative approaches that maintain effectiveness under resource limitations. The convergence phenomenon identified across multiple studies suggests that traditional DPO evaluation requires reconsideration in contexts where preference data availability is constrained, as statistical equivalence may reflect algorithmic limitations rather than genuine performance parity. Future research must address these scaling limitations through novel approaches that can extract meaningful preference signals from limited datasets, whilst acknowledging that current DPO variants may prove inadequate for resource-constrained deployment scenarios.

\section{Bias in AI System Evaluation}

The reliability of AI system assessment has emerged as a fundamental challenge in contemporary machine learning research, with systematic biases undermining the credibility of evaluation frameworks across domains. Position bias, consistency issues, and reliability challenges in automated assessment represent critical methodological limitations that traditional evaluation approaches consistently fail to address \cite{li2023position_bias_portia}. These biases manifest most prominently in LLM-as-a-Judge evaluation scenarios, where automated systems exhibit systematic preferences that compromise assessment validity. Li et al. demonstrate that large language models consistently favour responses in specific positions during pairwise comparisons, regardless of content quality, creating assessment artifacts that obscure genuine performance differences.

The pervasive nature of evaluation bias extends beyond simple position preferences to encompass multiple dimensions of systematic assessment failure. The PORTIA framework reveals that evaluator models demonstrate positional inconsistency rates exceeding 40\% across diverse comparison scenarios, indicating fundamental reliability limitations in current automated assessment approaches \cite{li2023position_bias_portia}. These inconsistencies become particularly pronounced when evaluating complex generation tasks, where quality differences may be subtle and require nuanced assessment capabilities that current evaluation frameworks cannot reliably provide. The research demonstrates that even state-of-the-art models exhibit significant variability in evaluation outcomes when identical content is presented in different orders, highlighting the systematic nature of assessment bias challenges.

Contemporary evaluation methodologies fail to incorporate reasoning transparency and explainability mechanisms essential for reliable AI assessment. Yang et al. identify that traditional evaluation approaches operate as "black box" systems that cannot provide justifiable rationales for assessment decisions, creating fundamental accountability gaps in AI system evaluation \cite{yang2025reasoning_bias_detector}. Their Reasoning-based Bias Detector framework demonstrates that incorporating explicit reasoning processes into evaluation methodologies can reduce assessment bias by up to 18.5\% whilst simultaneously improving evaluation consistency by 10.9\%. This finding establishes a clear connection between reasoning transparency and evaluation reliability, suggesting that opaque assessment processes contribute significantly to systematic bias propagation.

The integration of reasoning-enhanced evaluation approaches represents a critical advancement in addressing evaluation bias challenges, particularly for complex multi-agent systems where coordination effectiveness requires nuanced assessment. The bias detection and self-correction capabilities demonstrated through reasoning-based approaches provide mechanisms for identifying and mitigating systematic assessment failures that conventional evaluation frameworks cannot detect \cite{yang2025reasoning_bias_detector}. These methodological improvements become essential when evaluating systems with subtle performance differences, as reasoning transparency enables distinguishing genuine capability variations from measurement artifacts introduced by evaluation bias.

Traditional evaluation metrics demonstrate fundamental inadequacy for capturing nuanced quality differences in AI-generated content, creating systematic gaps between assessment outcomes and genuine performance capabilities. Seth and Sankarapu establish that current evaluation practices suffer from fragmented, subjective, and manipulation-prone characteristics that compromise assessment reliability across domains \cite{seth2025xai_metrics_gap}. Their analysis reveals that evaluation frameworks lack standardised reliability metrics, creating conditions where assessment outcomes reflect methodological limitations rather than genuine system performance differences. This measurement inadequacy becomes critical when regulatory compliance requires reliable assessment capabilities, as current evaluation approaches cannot provide the consistency necessary for systematic performance verification.

The absence of ground truth references in many evaluation scenarios compounds these measurement challenges, as comparative assessment approaches must rely on relative quality judgments that inherit evaluator biases. The systematic nature of these bias patterns suggests that conventional inter-rater reliability approaches prove inadequate for addressing evaluation challenges in AI system assessment \cite{seth2025xai_metrics_gap}. Research demonstrates that bias propagation occurs consistently across different evaluation contexts, indicating that assessment limitations represent fundamental methodological challenges rather than domain-specific measurement problems. This finding has significant implications for multi-agent system evaluation, where coordination effectiveness assessment requires frameworks capable of detecting subtle performance variations whilst avoiding systematic bias artifacts.

The statistical equivalence observed in the present study ($F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$) demonstrates the practical implications of evaluation bias limitations in multi-agent system assessment. This finding aligns precisely with the bias detection literature, which identifies that systematic assessment challenges can obscure genuine performance differences whilst creating false equivalence between system variants. The negligible effect size particularly reflects the evaluation framework limitations identified by Seth and Sankarapu, where measurement inadequacy prevents detection of meaningful system differences \cite{seth2025xai_metrics_gap}. The statistical outcome suggests that addressing evaluation bias challenges requires methodological innovations that can distinguish genuine performance equivalence from assessment artifact interference.

The Hybrid prompting methodology developed in the present study directly addresses these evaluation bias challenges through reasoning-enhanced assessment approaches. By incorporating explicit reasoning processes into evaluation frameworks, the methodology provides transparency mechanisms that enable bias detection and mitigation whilst maintaining assessment reliability. The approach demonstrates practical application of reasoning-based evaluation principles identified in the bias detection literature, establishing a methodological foundation for reliable multi-agent system assessment that can distinguish genuine coordination effectiveness from evaluation bias artifacts.

Addressing evaluation bias challenges requires comprehensive methodological reform that incorporates reasoning transparency, bias detection capabilities, and reliability verification mechanisms into assessment frameworks. The literature demonstrates clear evidence that traditional evaluation approaches systematically fail to provide reliable assessment capabilities for complex AI systems, necessitating novel methodologies specifically designed to address bias-related measurement limitations. Future research must prioritise developing evaluation frameworks capable of maintaining assessment reliability whilst providing the transparency necessary for systematic bias detection and mitigation, acknowledging that current evaluation practices may obscure rather than reveal genuine system performance differences.

\section{Email Generation Assessment Challenges}

The evaluation of professional communication generation presents unique challenges that distinguish it fundamentally from general natural language generation assessment. Email and business communication domains require evaluation frameworks capable of measuring pragmatic effectiveness, recipient appropriateness, and contextual sensitivityâ€”dimensions that conventional NLG metrics systematically fail to capture \cite{li2024llm_nlg_evaluation}. The domain-specific nature of professional communication introduces evaluation complexities that extend beyond traditional concerns of fluency, coherence, and factual accuracy, necessitating novel assessment approaches designed specifically for workplace communication contexts.

Professional writing quality assessment represents a particularly intractable evaluation challenge, as quality judgments depend heavily on subjective criteria and domain expertise that automated metrics cannot readily capture. Chakrabarty et al. demonstrate that even state-of-the-art language models capable of sophisticated reasoning tasks perform barely above random baselines when evaluating writing quality \cite{chakrabarty2025writing_quality}. Their comprehensive Writing Quality Benchmark reveals that conventional automatic metrics fail to correlate meaningfully with expert human judgments across 4,729 professional writing assessments. This finding highlights the fundamental inadequacy of current evaluation frameworks for measuring the pragmatic dimensions that define effective professional communication, including persuasiveness, recipient engagement, and contextual appropriateness.

The measurement challenges become more pronounced when considering the multi-dimensional nature of email effectiveness, where traditional NLG evaluation approaches prove systematically inadequate. Email generation success depends on factors that conventional metrics cannot assess: sender-recipient relationship dynamics, communication objectives, organisational context, and temporal appropriateness. The Zhang and Tetreault investigation of email subject line generation reveals that email communication exhibits "extremely abstractive" properties that differentiate it significantly from news headline generation or document summarisation tasks \cite{zhang2019email_subject}. Their empirical analysis demonstrates that evaluation metrics effective for other text generation domains produce misleading assessments when applied to email contexts, as they cannot capture the unique communicative objectives that characterise professional electronic correspondence.

Domain-specific evaluation limitations extend beyond individual metric inadequacies to fundamental methodological challenges in assessment framework design. Gehrmann et al. identify systematic obstacles in NLG evaluation practices that compound significantly when applied to professional communication contexts \cite{gehrmann2022nlg_evaluation_obstacles}. Their comprehensive survey reveals that current evaluation approaches rely primarily on surface-level features that neural generation models can easily satisfy whilst failing to assess deeper pragmatic qualities essential for effective communication. The research demonstrates that conventional evaluation practices cannot distinguish between text that appears professionally appropriate and communication that achieves genuine workplace effectiveness, creating a critical gap between assessment outcomes and real-world utility.

The recipient-centered nature of email effectiveness presents another dimension of evaluation complexity that current frameworks cannot systematically address. Professional email success depends fundamentally on recipient response and behaviour changeâ€”outcomes that require longitudinal assessment approaches absent from conventional NLG evaluation methodologies. This temporal dimension of communication effectiveness cannot be captured through immediate post-generation assessment, as email utility manifests through recipient engagement, task completion, and relationship maintenance over extended time periods. The assessment challenge becomes particularly acute when evaluating emails designed for specific organisational contexts, where effectiveness depends on cultural norms, hierarchical relationships, and institutional communication practices that automated metrics cannot reliably measure.

Current evaluation frameworks also fail to address the ethical and professional appropriateness dimensions that define acceptable workplace communication. Professional email generation requires adherence to organisational policies, legal compliance, and cultural sensitivity standards that extend beyond linguistic competence into domain-specific knowledge areas. The assessment of these dimensions requires evaluation approaches capable of measuring regulatory compliance, organisational alignment, and professional appropriatenessâ€”factors that conventional NLG metrics cannot systematically evaluate. This limitation becomes critical when deploying automated email generation systems in professional contexts, where inappropriate communication can produce significant organisational and legal consequences that traditional evaluation approaches cannot predict or prevent.

The statistical equivalence observed across different email generation approaches in the present study ($F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$) exemplifies these measurement inadequacies. The negligible effect size suggests either genuine performance equivalence or, more likely, evaluation framework limitations that obscure meaningful differences in professional communication quality. This statistical outcome parallels the broader challenges identified in professional writing assessment, where conventional metrics fail to detect quality differences that domain experts recognise consistently. The finding highlights the urgent need for evaluation approaches specifically designed to capture the multi-dimensional nature of professional communication effectiveness, moving beyond traditional NLG assessment paradigms toward frameworks that can measure pragmatic impact and recipient-centered outcomes.

Addressing these domain-specific evaluation challenges requires developing assessment methodologies that incorporate recipient feedback, longitudinal effectiveness measures, and professional appropriateness criteria into comprehensive evaluation frameworks. The literature demonstrates clear evidence that conventional NLG evaluation approaches prove systematically inadequate for professional communication contexts, necessitating novel methodologies designed specifically for workplace communication assessment. Future research must prioritise the development of evaluation frameworks capable of measuring the pragmatic dimensions that define effective professional communication, whilst acknowledging that current assessment approaches may obscure rather than reveal genuine performance differences in email generation systems.

\section{Research Positioning and Contributions}

The synthesis of evaluation framework limitations across multi-agent coordination, DPO performance under data constraints, evaluation bias mitigation, and email generation assessment reveals a critical convergence of research gaps that current AI system evaluation approaches systematically fail to address. The literature demonstrates that these challenges are not isolated methodological limitations but interconnected problems requiring integrated solutions that can simultaneously handle coordination complexity, statistical equivalence detection, bias mitigation, and domain-specific quality assessment. This convergence creates a significant opportunity for research that addresses multiple evaluation framework inadequacies through unified methodological innovations.

Contemporary AI system evaluation frameworks exhibit fundamental inability to distinguish genuine system differences from measurement artifacts when multiple evaluation challenges compound simultaneously. The statistical equivalence finding of $F(2,747) = 0.329$, $p = 0.720$, $\eta^2 = 0.001$ observed across three system variants exemplifies this broader evaluation crisis, where coordination effectiveness assessment, data constraint limitations, evaluation bias, and domain-specific measurement inadequacies combine to create conditions where meaningful performance differences become indistinguishable from random variation. Ferrag et al. identify this as a systematic challenge affecting autonomous AI agent evaluation across domains, noting that current assessment approaches fail to provide reliable measurement capabilities when multiple evaluation complexities interact \cite{ferrag2025comprehensive_ai_review}.

The evaluation framework gaps create particular challenges for multi-agent systems operating under realistic deployment constraints, where limited training data, coordination complexity, and domain-specific assessment requirements converge to produce conditions that exceed current evaluation methodology capabilities. The literature reveals that addressing these interconnected challenges requires comprehensive assessment frameworks that integrate multiple evaluation dimensions whilst maintaining statistical sophistication necessary for detecting subtle performance differences. The AILuminate benchmark development demonstrates the critical need for standardised evaluation approaches capable of handling complex AI system assessment scenarios, emphasising that evaluation framework limitations represent fundamental barriers to reliable AI system deployment \cite{ghosh2025ailuminate_benchmark}.

Against this backdrop of identified research gaps, the present study contributes three novel approaches that directly address the convergent evaluation challenges identified in the literature. First, the multi-agent framework addresses coordination evaluation gaps through a three-agent architecture specifically designed to enable systematic assessment of inter-agent collaboration effectiveness whilst avoiding the component-level evaluation limitations that plague current multi-agent system assessment. The framework demonstrates practical application of specialised evaluation approaches identified as necessary in the multi-agent literature, providing empirical evidence for coordination assessment methodologies that can distinguish between successful collaboration and coincidental task completion.

Second, the DPO comparison methodology provides empirical evidence for statistical equivalence under data constraints, directly addressing the critical gap in understanding preference optimisation limitations when datasets fall below threshold sizes. The study's demonstration that three DPO variants exhibit statistical performance equivalence with $\eta^2 = 0.001$ across 400-425 preference pairs contributes essential empirical validation of theoretical predictions about DPO scaling limitations. This finding provides practical guidance for DPO deployment decisions under resource constraints whilst establishing empirical boundaries for preference optimisation effectiveness that the literature has identified but not systematically demonstrated.

Third, the Hybrid evaluation methodology addresses evaluation bias challenges through reasoning-enhanced assessment approaches that provide transparency mechanisms essential for reliable multi-agent system evaluation. The methodology demonstrates practical application of reasoning-based bias mitigation principles identified in the evaluation bias literature, establishing a methodological foundation for systematic bias detection and correction in complex AI system assessment scenarios. This approach directly addresses the accountability gaps in automated evaluation whilst providing mechanisms for distinguishing genuine coordination effectiveness from assessment artifacts.

These contributions collectively demonstrate that integrated evaluation approaches can address multiple framework limitations simultaneously, providing methodological innovations that advance both theoretical understanding and practical deployment capabilities for multi-agent AI systems operating under realistic constraints. The study establishes empirical evidence that comprehensive evaluation frameworks can maintain assessment reliability whilst addressing the interconnected challenges that current evaluation approaches systematically fail to handle, contributing essential methodological foundations for future research in constrained AI system evaluation.
