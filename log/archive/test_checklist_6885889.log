Verifying PyTorch installation...
PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU count: 4
Running main.py with native model parallelism...
INFO 05-30 12:36:02 [__init__.py:239] Automatically detected platform cuda.
[INFO] Creating Checklist

--- Opening and reading: 1|deepseek-r1-14b.txt ---

--- Opening and reading: 3|llama-2-13b.txt ---

--- Opening and reading: 1|deepseek-r1-32b.txt ---

--- Opening and reading: 3|gemma-2-27b.txt ---

--- Opening and reading: 1|gemma-2-27b.txt ---

--- Opening and reading: 1|deepseek-r1-7b.txt ---

--- Opening and reading: 3|deepseek-r1-7b.txt ---

--- Opening and reading: 4|llama-2-7b.txt ---

--- Opening and reading: 3|llama-2-7b.txt ---

--- Opening and reading: 4|deepseek-r1-32b.txt ---

--- Opening and reading: 2|gemma-2-27b.txt ---

--- Opening and reading: 4|llama-3-70b.txt ---

--- Opening and reading: 1|llama-3-70b.txt ---

--- Opening and reading: 3|llama-3-70b.txt ---

--- Opening and reading: 3|deepseek-r1-32b.txt ---

--- Opening and reading: 4|deepseek-r1-7b.txt ---

--- Opening and reading: 2|deepseek-r1-32b.txt ---

--- Opening and reading: 3|gemma-2-9b.txt ---

--- Opening and reading: 2|llama-2-7b.txt ---

--- Opening and reading: 3|deepseek-r1-1.5b.txt ---

--- Opening and reading: 4|gemma-2-9b.txt ---

--- Opening and reading: 4|gemma-2-27b.txt ---

--- Opening and reading: 4|llama-2-13b.txt ---

--- Opening and reading: 4|deepseek-r1-1.5b.txt ---

--- Opening and reading: 2|gemma-2-9b.txt ---

--- Opening and reading: 2|llama-3-70b.txt ---

--- Opening and reading: 4|deepseek-r1-70b.txt ---

--- Opening and reading: 3|deepseek-r1-70b.txt ---

--- Opening and reading: 2|deepseek-r1-7b.txt ---

--- Opening and reading: 4|deepseek-r1-14b.txt ---

--- Opening and reading: 2|deepseek-r1-1.5b.txt ---

--- Opening and reading: 1|deepseek-r1-70b.txt ---

--- Opening and reading: 3|deepseek-r1-14b.txt ---

--- Opening and reading: 2|deepseek-r1-70b.txt ---

--- Opening and reading: 2|deepseek-r1-14b.txt ---

--- Opening and reading: 1|deepseek-r1-1.5b.txt ---

--- Opening and reading: 1|llama-2-7b.txt ---

--- Opening and reading: 2|llama-2-13b.txt ---

--- Opening and reading: 1|llama-2-13b.txt ---

--- Opening and reading: 1|gemma-2-9b.txt ---
Loading model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
INFO 05-30 12:37:23 [config.py:1770] Defaulting to use mp for distributed inference
INFO 05-30 12:37:23 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 05-30 12:37:40 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 12:37:43 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir='../downloaded_models', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
INFO 05-30 12:37:43 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_64dc0395'), local_subscribe_addr='ipc:///tmp/job.6885889/0bb3c3b9-8f29-43c3-92f0-655a521cd668', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 05-30 12:37:58 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 12:37:58 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 12:37:58 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 12:37:58 [__init__.py:239] Automatically detected platform cuda.
WARNING 05-30 12:38:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1980a7aae0>
WARNING 05-30 12:38:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f746a9e95b0>
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e1cfffaa'), local_subscribe_addr='ipc:///tmp/job.6885889/4d41f33f-293c-47e2-a0d0-d1799eda24f3', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 05-30 12:38:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f657478f5f0>
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_590f4309'), local_subscribe_addr='ipc:///tmp/job.6885889/4951c873-c9cd-4ef9-a006-f325d4e14820', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c0e175f1'), local_subscribe_addr='ipc:///tmp/job.6885889/6d74e3a2-cb04-4503-b8cb-c45a4dc6b477', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 05-30 12:38:25 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f612419edb0>
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:25 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_781a3071'), local_subscribe_addr='ipc:///tmp/job.6885889/7fb7626e-8761-4543-9a9f-58be01775153', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:28 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:28 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:28 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:28 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:28 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:28 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:28 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:28 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:36 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:36 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:36 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:36 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:36 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_dc927abe'), local_subscribe_addr='ipc:///tmp/job.6885889/79eec658-368b-4b47-b699-adadc588dac1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:36 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=30198)[0;0m WARNING 05-30 12:38:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:36 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=30199)[0;0m WARNING 05-30 12:38:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:36 [parallel_state.py:1004] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=30200)[0;0m WARNING 05-30 12:38:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:36 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:36 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=30201)[0;0m WARNING 05-30 12:38:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:36 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:36 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:36 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:36 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:38:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:38:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:38:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:38:39 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [01:17<09:00, 77.24s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [02:06<06:03, 60.52s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [02:59<04:47, 57.49s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [04:03<04:00, 60.07s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [04:39<02:33, 51.25s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [06:07<02:07, 63.55s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [07:13<01:04, 64.59s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [07:37<00:00, 51.67s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [07:37<00:00, 57.22s/it]
[1;36m(VllmWorker rank=0 pid=30198)[0;0m 
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:46:17 [loader.py:458] Loading weights took 457.83 seconds
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:46:17 [loader.py:458] Loading weights took 457.28 seconds
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:46:17 [loader.py:458] Loading weights took 458.48 seconds
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:46:18 [loader.py:458] Loading weights took 457.89 seconds
[1;36m(VllmWorker rank=3 pid=30201)[0;0m WARNING 05-30 12:46:19 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=1 pid=30199)[0;0m WARNING 05-30 12:46:19 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=0 pid=30198)[0;0m WARNING 05-30 12:46:19 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=2 pid=30200)[0;0m WARNING 05-30 12:46:19 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=2 pid=30200)[0;0m WARNING 05-30 12:46:21 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=1 pid=30199)[0;0m WARNING 05-30 12:46:21 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=3 pid=30201)[0;0m WARNING 05-30 12:46:21 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=0 pid=30198)[0;0m WARNING 05-30 12:46:21 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:46:25 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 466.196883 seconds
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:46:25 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 466.222106 seconds
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:46:25 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 466.223410 seconds
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:46:25 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 466.202730 seconds
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:48:35 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:48:35 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:48:35 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:48:35 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:48:35 [backends.py:430] Dynamo bytecode transform time: 128.84 s
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:48:35 [backends.py:430] Dynamo bytecode transform time: 128.84 s
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:48:35 [backends.py:430] Dynamo bytecode transform time: 128.84 s
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:48:35 [backends.py:430] Dynamo bytecode transform time: 128.84 s
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:49:33 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 52.390 s
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:49:33 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 52.402 s
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:49:33 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 52.401 s
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:49:33 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 52.400 s
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:50:03 [monitor.py:33] torch.compile takes 128.84 s in total
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:50:03 [monitor.py:33] torch.compile takes 128.84 s in total
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:50:03 [monitor.py:33] torch.compile takes 128.84 s in total
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:50:03 [monitor.py:33] torch.compile takes 128.84 s in total
INFO 05-30 12:50:12 [kv_cache_utils.py:634] GPU KV cache size: 895,600 tokens
INFO 05-30 12:50:12 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.83x
INFO 05-30 12:50:12 [kv_cache_utils.py:634] GPU KV cache size: 893,296 tokens
INFO 05-30 12:50:12 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.82x
INFO 05-30 12:50:12 [kv_cache_utils.py:634] GPU KV cache size: 893,296 tokens
INFO 05-30 12:50:12 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.82x
INFO 05-30 12:50:12 [kv_cache_utils.py:634] GPU KV cache size: 895,600 tokens
INFO 05-30 12:50:12 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.83x
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:52:29 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:52:29 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:52:29 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:52:29 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=30201)[0;0m INFO 05-30 12:52:32 [gpu_model_runner.py:1686] Graph capturing finished in 140 secs, took 1.03 GiB
[1;36m(VllmWorker rank=1 pid=30199)[0;0m INFO 05-30 12:52:32 [gpu_model_runner.py:1686] Graph capturing finished in 140 secs, took 1.03 GiB
[1;36m(VllmWorker rank=0 pid=30198)[0;0m INFO 05-30 12:52:32 [gpu_model_runner.py:1686] Graph capturing finished in 140 secs, took 1.03 GiB
[1;36m(VllmWorker rank=2 pid=30200)[0;0m INFO 05-30 12:52:32 [gpu_model_runner.py:1686] Graph capturing finished in 140 secs, took 1.03 GiB
INFO 05-30 12:52:33 [core.py:159] init engine (profile, create kv cache, warmup model) took 367.02 seconds
INFO 05-30 12:52:34 [core_client.py:439] Core engine process 0 ready.
Model loaded in 931.54 seconds
1|deepseek-r1-14b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.47s/it, est. speed input: 52.83 toks/s, output: 15.68 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.47s/it, est. speed input: 52.83 toks/s, output: 15.68 toks/s]
Generated response in 24.29 seconds
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/mnt/parscratch/users/liq23wr/dissertation/test_checklist.py", line 116, in <module>
    checklist_dict = create_checklist(response_file=None, 
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/liq23wr/dissertation/evaluation/checklist.py", line 46, in create_checklist
    checklist_dict[key] = dict(formatted_response)
                          ^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: dictionary update sequence element #0 has length 3; 2 is required
slurmstepd: error: *** JOB 6885889 ON gpu12 CANCELLED AT 2025-05-30T13:11:05 ***
