Verifying PyTorch installation...
PyTorch version: 2.6.0+cu124
CUDA available: True
CUDA version: 12.4
GPU count: 4
Running main.py with native model parallelism...
INFO 05-30 13:13:59 [__init__.py:239] Automatically detected platform cuda.
[INFO] Creating Checklist

--- Opening and reading: 1|deepseek-r1-14b.txt ---

--- Opening and reading: 3|llama-2-13b.txt ---

--- Opening and reading: 1|deepseek-r1-32b.txt ---

--- Opening and reading: 3|gemma-2-27b.txt ---

--- Opening and reading: 1|gemma-2-27b.txt ---

--- Opening and reading: 1|deepseek-r1-7b.txt ---

--- Opening and reading: 3|deepseek-r1-7b.txt ---

--- Opening and reading: 4|llama-2-7b.txt ---

--- Opening and reading: 3|llama-2-7b.txt ---

--- Opening and reading: 4|deepseek-r1-32b.txt ---

--- Opening and reading: 2|gemma-2-27b.txt ---

--- Opening and reading: 4|llama-3-70b.txt ---

--- Opening and reading: 1|llama-3-70b.txt ---

--- Opening and reading: 3|llama-3-70b.txt ---

--- Opening and reading: 3|deepseek-r1-32b.txt ---

--- Opening and reading: 4|deepseek-r1-7b.txt ---

--- Opening and reading: 2|deepseek-r1-32b.txt ---

--- Opening and reading: 3|gemma-2-9b.txt ---

--- Opening and reading: 2|llama-2-7b.txt ---

--- Opening and reading: 3|deepseek-r1-1.5b.txt ---

--- Opening and reading: 4|gemma-2-9b.txt ---

--- Opening and reading: 4|gemma-2-27b.txt ---

--- Opening and reading: 4|llama-2-13b.txt ---

--- Opening and reading: 4|deepseek-r1-1.5b.txt ---

--- Opening and reading: 2|gemma-2-9b.txt ---

--- Opening and reading: 2|llama-3-70b.txt ---

--- Opening and reading: 4|deepseek-r1-70b.txt ---

--- Opening and reading: 3|deepseek-r1-70b.txt ---

--- Opening and reading: 2|deepseek-r1-7b.txt ---

--- Opening and reading: 4|deepseek-r1-14b.txt ---

--- Opening and reading: 2|deepseek-r1-1.5b.txt ---

--- Opening and reading: 1|deepseek-r1-70b.txt ---

--- Opening and reading: 3|deepseek-r1-14b.txt ---

--- Opening and reading: 2|deepseek-r1-70b.txt ---

--- Opening and reading: 2|deepseek-r1-14b.txt ---

--- Opening and reading: 1|deepseek-r1-1.5b.txt ---

--- Opening and reading: 1|llama-2-7b.txt ---

--- Opening and reading: 2|llama-2-13b.txt ---

--- Opening and reading: 1|llama-2-13b.txt ---

--- Opening and reading: 1|gemma-2-9b.txt ---
Loading model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
INFO 05-30 13:14:57 [config.py:1770] Defaulting to use mp for distributed inference
INFO 05-30 13:14:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 05-30 13:15:12 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 13:15:15 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir='../downloaded_models', load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
INFO 05-30 13:15:15 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_3c239448'), local_subscribe_addr='ipc:///tmp/job.6886035/ba7ac6d5-4b01-4b86-b402-2f2730a98056', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 05-30 13:15:30 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 13:15:30 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 13:15:30 [__init__.py:239] Automatically detected platform cuda.
INFO 05-30 13:15:30 [__init__.py:239] Automatically detected platform cuda.
WARNING 05-30 13:15:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f217af75130>
WARNING 05-30 13:15:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9dd596ddf0>
WARNING 05-30 13:15:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe8e29678f0>
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:15:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_61bec253'), local_subscribe_addr='ipc:///tmp/job.6886035/192a44b9-be86-4f99-8887-febba391f458', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:15:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a905c5a'), local_subscribe_addr='ipc:///tmp/job.6886035/40949bbd-ed8e-411f-a279-0563fa410dfc', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 05-30 13:15:51 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f3c5eee1a30>
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:15:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b90a0ea9'), local_subscribe_addr='ipc:///tmp/job.6886035/bb6f60f8-e531-4f48-a606-5dc40690f2ac', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:15:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e863b951'), local_subscribe_addr='ipc:///tmp/job.6886035/0c401422-fd7f-4295-b034-f9890e786f64', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:15:54 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:15:54 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:15:54 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:15:54 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:15:54 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:15:54 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:15:54 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:15:54 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:16:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:16:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:16:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /users/liq23wr/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:01 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_fc6ad196'), local_subscribe_addr='ipc:///tmp/job.6886035/37969735-fb41-41c4-8977-d447265e077a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:01 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:01 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=31956)[0;0m WARNING 05-30 13:16:01 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:16:01 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:16:01 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=31957)[0;0m WARNING 05-30 13:16:01 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:16:01 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:16:01 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=31959)[0;0m WARNING 05-30 13:16:01 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:16:01 [parallel_state.py:1004] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:16:01 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=31958)[0;0m WARNING 05-30 13:16:01 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:16:02 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:16:02 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:16:02 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:02 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:16:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:16:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:16:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:16:05 [weight_utils.py:265] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:14<01:39, 14.15s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:18<00:51,  8.61s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:22<00:32,  6.51s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:31<00:30,  7.51s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:35<00:18,  6.04s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  75% Completed | 6/8 [02:00<01:05, 32.78s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards:  88% Completed | 7/8 [03:01<00:42, 42.08s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:03<00:00, 29.45s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:03<00:00, 22.96s/it]
[1;36m(VllmWorker rank=0 pid=31956)[0;0m 
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:19:10 [loader.py:458] Loading weights took 183.79 seconds
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:19:10 [loader.py:458] Loading weights took 183.56 seconds
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:19:10 [loader.py:458] Loading weights took 184.47 seconds
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:19:10 [loader.py:458] Loading weights took 184.27 seconds
[1;36m(VllmWorker rank=1 pid=31957)[0;0m WARNING 05-30 13:19:11 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=0 pid=31956)[0;0m WARNING 05-30 13:19:11 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=2 pid=31958)[0;0m WARNING 05-30 13:19:11 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=3 pid=31959)[0;0m WARNING 05-30 13:19:11 [marlin_utils_fp8.py:54] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[1;36m(VllmWorker rank=3 pid=31959)[0;0m WARNING 05-30 13:19:12 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=1 pid=31957)[0;0m WARNING 05-30 13:19:12 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=0 pid=31956)[0;0m WARNING 05-30 13:19:12 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=2 pid=31958)[0;0m WARNING 05-30 13:19:12 [kv_cache.py:128] Using Q scale 1.0 and prob scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure Q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:19:15 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 191.433129 seconds
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:19:15 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 191.421653 seconds
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:19:15 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 191.507617 seconds
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:19:15 [gpu_model_runner.py:1347] Model loading took 8.0869 GiB and 191.458442 seconds
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:21:00 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:21:00 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:21:00 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:21:00 [backends.py:420] Using cache directory: /users/liq23wr/.cache/vllm/torch_compile_cache/905a0c1011/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:21:00 [backends.py:430] Dynamo bytecode transform time: 104.10 s
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:21:00 [backends.py:430] Dynamo bytecode transform time: 104.10 s
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:21:00 [backends.py:430] Dynamo bytecode transform time: 104.10 s
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:21:00 [backends.py:430] Dynamo bytecode transform time: 104.11 s
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:21:55 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 50.574 s
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:21:55 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 50.662 s
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:21:55 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 50.783 s
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:21:55 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 51.202 s
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:22:24 [monitor.py:33] torch.compile takes 104.11 s in total
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:22:24 [monitor.py:33] torch.compile takes 104.10 s in total
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:22:24 [monitor.py:33] torch.compile takes 104.10 s in total
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:22:24 [monitor.py:33] torch.compile takes 104.10 s in total
INFO 05-30 13:22:29 [kv_cache_utils.py:634] GPU KV cache size: 895,600 tokens
INFO 05-30 13:22:29 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.83x
INFO 05-30 13:22:29 [kv_cache_utils.py:634] GPU KV cache size: 893,296 tokens
INFO 05-30 13:22:29 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.82x
INFO 05-30 13:22:29 [kv_cache_utils.py:634] GPU KV cache size: 893,296 tokens
INFO 05-30 13:22:29 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.82x
INFO 05-30 13:22:29 [kv_cache_utils.py:634] GPU KV cache size: 895,600 tokens
INFO 05-30 13:22:29 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 6.83x
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:24:41 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:24:41 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:24:41 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:24:41 [custom_all_reduce.py:195] Registering 8643 cuda graph addresses
[1;36m(VllmWorker rank=0 pid=31956)[0;0m INFO 05-30 13:24:42 [gpu_model_runner.py:1686] Graph capturing finished in 133 secs, took 1.03 GiB
[1;36m(VllmWorker rank=1 pid=31957)[0;0m INFO 05-30 13:24:42 [gpu_model_runner.py:1686] Graph capturing finished in 133 secs, took 1.03 GiB
[1;36m(VllmWorker rank=2 pid=31958)[0;0m INFO 05-30 13:24:42 [gpu_model_runner.py:1686] Graph capturing finished in 133 secs, took 1.03 GiB
[1;36m(VllmWorker rank=3 pid=31959)[0;0m INFO 05-30 13:24:42 [gpu_model_runner.py:1686] Graph capturing finished in 133 secs, took 1.03 GiB
INFO 05-30 13:24:42 [core.py:159] init engine (profile, create kv cache, warmup model) took 326.91 seconds
INFO 05-30 13:24:43 [core_client.py:439] Core engine process 0 ready.
Model loaded in 602.95 seconds
1|deepseek-r1-14b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.68s/it, est. speed input: 29.05 toks/s, output: 16.64 toks/s]Processed prompts: 100%|██████████| 1/1 [00:42<00:00, 42.70s/it, est. speed input: 29.05 toks/s, output: 16.64 toks/s]
Generated response in 42.71 seconds
3|llama-2-13b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.95s/it, est. speed input: 59.43 toks/s, output: 17.38 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.95s/it, est. speed input: 59.43 toks/s, output: 17.38 toks/s]
Generated response in 20.95 seconds
1|deepseek-r1-32b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.03s/it, est. speed input: 65.16 toks/s, output: 17.08 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.03s/it, est. speed input: 65.16 toks/s, output: 17.08 toks/s]
Generated response in 19.03 seconds
3|gemma-2-27b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:50<00:00, 50.94s/it, est. speed input: 24.65 toks/s, output: 17.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:50<00:00, 50.95s/it, est. speed input: 24.65 toks/s, output: 17.53 toks/s]
Generated response in 50.95 seconds
1|gemma-2-27b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:36<00:00, 36.96s/it, est. speed input: 33.98 toks/s, output: 17.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:36<00:00, 36.96s/it, est. speed input: 33.98 toks/s, output: 17.50 toks/s]
Generated response in 36.97 seconds
1|deepseek-r1-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.65s/it, est. speed input: 57.26 toks/s, output: 17.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.66s/it, est. speed input: 57.26 toks/s, output: 17.69 toks/s]
Generated response in 21.66 seconds
3|deepseek-r1-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.36s/it, est. speed input: 55.46 toks/s, output: 17.75 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.36s/it, est. speed input: 55.46 toks/s, output: 17.75 toks/s]
Generated response in 22.51 seconds
JSON block not found in the expected format.
4|llama-2-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it, est. speed input: 68.71 toks/s, output: 17.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.12s/it, est. speed input: 68.71 toks/s, output: 17.49 toks/s]
Generated response in 18.12 seconds
3|llama-2-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.63s/it, est. speed input: 66.81 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.63s/it, est. speed input: 66.81 toks/s, output: 17.55 toks/s]
Generated response in 18.64 seconds
4|deepseek-r1-32b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.67s/it, est. speed input: 57.21 toks/s, output: 17.49 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.67s/it, est. speed input: 57.21 toks/s, output: 17.49 toks/s]
Generated response in 21.68 seconds
2|gemma-2-27b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.45s/it, est. speed input: 26.47 toks/s, output: 17.62 toks/s]Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.45s/it, est. speed input: 26.47 toks/s, output: 17.62 toks/s]
Generated response in 47.45 seconds
4|llama-3-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.28s/it, est. speed input: 55.88 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.28s/it, est. speed input: 55.88 toks/s, output: 17.55 toks/s]
Generated response in 22.29 seconds
1|llama-3-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.63s/it, est. speed input: 60.34 toks/s, output: 17.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.63s/it, est. speed input: 60.34 toks/s, output: 17.50 toks/s]
Generated response in 20.64 seconds
3|llama-3-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.33s/it, est. speed input: 71.85 toks/s, output: 17.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.33s/it, est. speed input: 71.85 toks/s, output: 17.60 toks/s]
Generated response in 17.33 seconds
3|deepseek-r1-32b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.19s/it, est. speed input: 21.31 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.19s/it, est. speed input: 21.31 toks/s, output: 17.55 toks/s]
Generated response in 58.20 seconds
4|deepseek-r1-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.93s/it, est. speed input: 65.50 toks/s, output: 17.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.93s/it, est. speed input: 65.50 toks/s, output: 17.54 toks/s]
Generated response in 18.93 seconds
2|deepseek-r1-32b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.20s/it, est. speed input: 64.59 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.20s/it, est. speed input: 64.59 toks/s, output: 17.55 toks/s]
Generated response in 19.20 seconds
JSON block not found in the expected format.
3|gemma-2-9b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.44s/it, est. speed input: 23.50 toks/s, output: 17.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.45s/it, est. speed input: 23.50 toks/s, output: 17.57 toks/s]
Generated response in 53.45 seconds
2|llama-2-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.79s/it, est. speed input: 69.97 toks/s, output: 17.59 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.79s/it, est. speed input: 69.97 toks/s, output: 17.59 toks/s]
Generated response in 17.80 seconds
3|deepseek-r1-1.5b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.33s/it, est. speed input: 55.52 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.33s/it, est. speed input: 55.52 toks/s, output: 17.55 toks/s]
Generated response in 22.34 seconds
4|gemma-2-9b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:46<00:00, 46.70s/it, est. speed input: 26.90 toks/s, output: 17.58 toks/s]Processed prompts: 100%|██████████| 1/1 [00:46<00:00, 46.70s/it, est. speed input: 26.90 toks/s, output: 17.58 toks/s]
Generated response in 46.70 seconds
4|gemma-2-27b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.11s/it, est. speed input: 21.61 toks/s, output: 17.60 toks/s]Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.11s/it, est. speed input: 21.61 toks/s, output: 17.60 toks/s]
Generated response in 58.12 seconds
4|llama-2-13b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.97s/it, est. speed input: 59.38 toks/s, output: 17.36 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.97s/it, est. speed input: 59.38 toks/s, output: 17.36 toks/s]
Generated response in 20.97 seconds
4|deepseek-r1-1.5b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.29s/it, est. speed input: 61.11 toks/s, output: 17.69 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.29s/it, est. speed input: 61.11 toks/s, output: 17.69 toks/s]
Generated response in 20.29 seconds
2|gemma-2-9b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.18s/it, est. speed input: 26.62 toks/s, output: 17.53 toks/s]Processed prompts: 100%|██████████| 1/1 [00:47<00:00, 47.18s/it, est. speed input: 26.62 toks/s, output: 17.53 toks/s]
Generated response in 47.19 seconds
2|llama-3-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it, est. speed input: 61.40 toks/s, output: 17.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.28s/it, est. speed input: 61.40 toks/s, output: 17.61 toks/s]
Generated response in 20.28 seconds
4|deepseek-r1-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it, est. speed input: 67.16 toks/s, output: 17.71 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.46s/it, est. speed input: 67.16 toks/s, output: 17.71 toks/s]
Generated response in 18.47 seconds
3|deepseek-r1-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.65s/it, est. speed input: 63.12 toks/s, output: 17.61 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.65s/it, est. speed input: 63.12 toks/s, output: 17.61 toks/s]
Generated response in 19.65 seconds
2|deepseek-r1-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.98s/it, est. speed input: 51.70 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.98s/it, est. speed input: 51.70 toks/s, output: 17.55 toks/s]
Generated response in 23.99 seconds
4|deepseek-r1-14b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.86s/it, est. speed input: 23.02 toks/s, output: 17.54 toks/s]Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.95s/it, est. speed input: 23.02 toks/s, output: 17.54 toks/s]
Generated response in 53.95 seconds
2|deepseek-r1-1.5b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.88s/it, est. speed input: 73.47 toks/s, output: 17.95 toks/s]Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.88s/it, est. speed input: 73.47 toks/s, output: 17.95 toks/s]
Generated response in 16.88 seconds
1|deepseek-r1-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.47s/it, est. speed input: 70.99 toks/s, output: 17.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.47s/it, est. speed input: 70.99 toks/s, output: 17.63 toks/s]
Generated response in 17.47 seconds
3|deepseek-r1-14b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.50s/it, est. speed input: 67.02 toks/s, output: 17.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.50s/it, est. speed input: 67.02 toks/s, output: 17.57 toks/s]
Generated response in 18.51 seconds
2|deepseek-r1-70b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it, est. speed input: 72.37 toks/s, output: 17.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.13s/it, est. speed input: 72.37 toks/s, output: 17.63 toks/s]
Generated response in 17.14 seconds
2|deepseek-r1-14b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.41s/it, est. speed input: 57.92 toks/s, output: 17.65 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.41s/it, est. speed input: 57.92 toks/s, output: 17.65 toks/s]
Generated response in 21.41 seconds
1|deepseek-r1-1.5b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.03s/it, est. speed input: 56.28 toks/s, output: 17.57 toks/s]Processed prompts: 100%|██████████| 1/1 [00:22<00:00, 22.03s/it, est. speed input: 56.28 toks/s, output: 17.57 toks/s]
Generated response in 22.03 seconds
1|llama-2-7b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.68s/it, est. speed input: 63.25 toks/s, output: 17.63 toks/s]Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.68s/it, est. speed input: 63.25 toks/s, output: 17.63 toks/s]
Generated response in 19.69 seconds
2|llama-2-13b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.84s/it, est. speed input: 69.80 toks/s, output: 17.55 toks/s]Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.84s/it, est. speed input: 69.80 toks/s, output: 17.55 toks/s]
Generated response in 17.84 seconds
1|llama-2-13b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.77s/it, est. speed input: 57.19 toks/s, output: 17.73 toks/s]Processed prompts: 100%|██████████| 1/1 [00:21<00:00, 21.77s/it, est. speed input: 57.19 toks/s, output: 17.73 toks/s]
Generated response in 21.77 seconds
1|gemma-2-9b
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|██████████| 1/1 [00:46<00:00, 46.06s/it, est. speed input: 27.27 toks/s, output: 17.50 toks/s]Processed prompts: 100%|██████████| 1/1 [00:46<00:00, 46.06s/it, est. speed input: 27.27 toks/s, output: 17.50 toks/s]
Generated response in 46.06 seconds
slurmstepd: error: *** JOB 6886035 ON gpu12 CANCELLED AT 2025-05-30T17:07:54 ***
