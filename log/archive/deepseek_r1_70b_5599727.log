Verifying PyTorch installation...
PyTorch version: 2.5.1+cu124
CUDA available: True
CUDA version: 12.4
GPU count: 3
GPU status before execution:
Sat Mar  8 06:02:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   31C    P0             61W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   29C    P0             58W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   31C    P0             64W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Running main.py with native model parallelism...
INFO 03-08 06:03:33 __init__.py:190] Automatically detected platform cuda.
Opening file: 01.txt
Loading model deepseek-ai/DeepSeek-R1-Distill-Llama-70B...
INFO 03-08 06:04:15 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.
INFO 03-08 06:04:15 config.py:1401] Defaulting to use mp for distributed inference
WARNING 03-08 06:04:15 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 03-08 06:04:15 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.
Traceback (most recent call last):
  File "/mnt/parscratch/users/liq23wr/dissertation/main.py", line 88, in <module>
    generate_responses(prompt_dict=prompt_dict,
  File "/mnt/parscratch/users/liq23wr/dissertation/main.py", line 56, in generate_responses
    llm = ModelInference(model_id=model_id, quantization='fp8')
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/liq23wr/dissertation/models/models.py", line 22, in __init__
    self.llm = LLM(
               ^^^^
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/utils.py", line 1051, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 242, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 481, in from_engine_args
    engine_config = engine_args.create_engine_config(usage_context)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1276, in create_engine_config
    config = VllmConfig(
             ^^^^^^^^^^^
  File "<string>", line 19, in __init__
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/config.py", line 3209, in __post_init__
    self.model_config.verify_with_parallel_config(self.parallel_config)
  File "/mnt/parscratch/users/liq23wr/anaconda/.envs/dis-venv3/lib/python3.12/site-packages/vllm/config.py", line 710, in verify_with_parallel_config
    raise ValueError(
ValueError: Total number of attention heads (64) must be divisible by tensor parallel size (3).
Execution failed with error code 1
GPU status after execution:
Sat Mar  8 06:04:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   31C    P0             61W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   29C    P0             58W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   30C    P0             60W /  500W |       1MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Job completed
